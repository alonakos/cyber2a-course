[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cyber2A training curriculum for Arctic-AI research ~ DRAFT ~",
    "section": "",
    "text": "Preface\nBased on the team’s intensive research and training efforts in AI for Arctic Science, we will develop our training curriculum to address three questions:\n\nWhat are the “must-know” topics in AI to help trainees that have little computer science and computing knowledge to quickly grasp the essence of AI and “AI thinking”?\nWhat is the best way to offer training so AI beginners can get hands-on AI experience and prepare for using and developing AI models to solve real-world research problems? and\nHow does one best address Arctic science-specific issues (e.g., identification of spatiotemporal autocorrelation and location-awareness) for the effective AI modeling of geophysical phenomena?"
  },
  {
    "objectID": "sections/intro-to-neural-networks.html#notes",
    "href": "sections/intro-to-neural-networks.html#notes",
    "title": "1  Intro to neural networks",
    "section": "1.1 Notes:",
    "text": "1.1 Notes:\nminimal to no coding. duration: 2 hrs"
  },
  {
    "objectID": "sections/intro-to-neural-networks.html#goal",
    "href": "sections/intro-to-neural-networks.html#goal",
    "title": "1  Intro to neural networks",
    "section": "1.2 Goal:",
    "text": "1.2 Goal:\nGain a high-level understanding of how a NN works. Understand main components of NN input data, layers, weights, targets, loss function, optimizers, train and test sets, …"
  },
  {
    "objectID": "sections/intro-to-neural-networks.html#breakdown",
    "href": "sections/intro-to-neural-networks.html#breakdown",
    "title": "1  Intro to neural networks",
    "section": "1.3 Breakdown:",
    "text": "1.3 Breakdown:\n\nIntroduction and Overview\n\nBrief history of NN\nImportance and applications in today’s world\nRelevance to Arctic science\n\nBasic concepts and terminology\n\nNeurons and layers: Input, Hidden, Output\nWeights and biases\nActivation functions: Sigmoid, ReLU, etc.\n\nHow Neural Networks Learn\n\nForward propagation: How input becomes output\nCost function: Measuring how “wrong” the network is\nBackpropagation: Adjusting weights and biases\nGradient descent and learning rate\n\nTypes of Neural Networks\n\nMLP\nCNN\nRNN/LSTM\nTransformer\n\nTraining, Validation, and Testing\n\nSplitting data: Why and how\nOverfitting and underfitting: Concepts and solutions\nRegularization techniques\n\nReal-world Example and Demonstration\n\nIntroduce frameworks (PyTorch, Tensorflow)\nsimple/interesting examples\nVisualization"
  },
  {
    "objectID": "sections/retrogressive-thaw-slumps.html#notes",
    "href": "sections/retrogressive-thaw-slumps.html#notes",
    "title": "2  Retrogressive thaw slumps",
    "section": "2.1 Notes",
    "text": "2.1 Notes\nduration: 2 hours"
  },
  {
    "objectID": "sections/retrogressive-thaw-slumps.html#goal",
    "href": "sections/retrogressive-thaw-slumps.html#goal",
    "title": "2  Retrogressive thaw slumps",
    "section": "2.2 Goal",
    "text": "2.2 Goal\nTo familiarize participants with the characteristics, significance, and challenges of Retrogressive Thaw Slumps data, and to set the foundation for its application in deep learning"
  },
  {
    "objectID": "sections/retrogressive-thaw-slumps.html#breakdown-sample",
    "href": "sections/retrogressive-thaw-slumps.html#breakdown-sample",
    "title": "2  Retrogressive thaw slumps",
    "section": "2.3 Breakdown (sample):",
    "text": "2.3 Breakdown (sample):\n\nIntroduction to Retrogressive Thaw Slumps\n\nWhat are Retrogressive Thaw Slumps?\nImportance in the context of arctic science and climate change\nVisual representation (images or videos, if available)\n\nCharacteristics of RTS Data\nChallenges with RTS Data\nPotential Applications of Deep Learning with RTS Data\nHands-on Exploration\n\nA brief interactive session where participants can view and explore sample RTS data (using tools like Jupyter notebooks)"
  },
  {
    "objectID": "sections/intro-to-pytorch.html#goal",
    "href": "sections/intro-to-pytorch.html#goal",
    "title": "3  Intorduction to PyTorch",
    "section": "3.1 Goal",
    "text": "3.1 Goal\nTo provide participants with a foundational understanding of PyTorch, its capabilities, and how it can be used to implement neural networks and process data, especially in the context of Retrogressive Thaw Slumps."
  },
  {
    "objectID": "sections/intro-to-pytorch.html#breakdown",
    "href": "sections/intro-to-pytorch.html#breakdown",
    "title": "3  Intorduction to PyTorch",
    "section": "3.2 Breakdown",
    "text": "3.2 Breakdown\n\nOverview of Deep Learning Frameworks\n\nBrief mention of popular frameworks: TensorFlow, Keras, etc.\nWhy PyTorch? Advantages and use cases\n\nPyTorch Basics\n\nTensors: Understanding the basic data structure in PyTorch\nOperations with tensors: Reshaping, slicing, mathematical operations\nGPU vs. CPU: How PyTorch utilizes hardware acceleration\n\nData in PyTorch\n\nDataset and DataLoader: Efficiently loading and batching data\nTransformations: Augmenting and preprocessing data\nConnecting the dots: How RTS data can be loaded and preprocessed in PyTorch\n\nModel Building in PyTorch (30 minutes)\n\nnn.Module: Creating custom neural network architectures\nLayers in PyTorch: Linear, Conv2D, RNN, etc.\nActivation functions: ReLU, Sigmoid, Tanh, etc.\n\nOptimizers, Loss Functions, and Schedulers\n\nLoss functions: MSE, CrossEntropy, etc.\nOptimizers: Adam, SGD, etc.\nLearning rate schedulers: StepLR, ReduceLROnPlateau, etc.\n\nTraining, Validation, and Testing Pipeline\n\nForward and backward propagation in PyTorch\nModel evaluation: Accuracy, loss, and other metrics\nOverfitting: Early stopping, dropout, and other regularization techniques\nA simple example: Training, validating, and testing a small neural network on sample data"
  },
  {
    "objectID": "sections/pytorch-hands-on-lab.html#notes",
    "href": "sections/pytorch-hands-on-lab.html#notes",
    "title": "4  PyTorch Hands-on Lab",
    "section": "4.1 Notes",
    "text": "4.1 Notes\n1 hours"
  },
  {
    "objectID": "sections/pytorch-hands-on-lab.html#goal",
    "href": "sections/pytorch-hands-on-lab.html#goal",
    "title": "4  PyTorch Hands-on Lab",
    "section": "4.2 Goal",
    "text": "4.2 Goal\nTo provide participants with practical experience in using PyTorch, allowing them to implement and experiment with the concepts introduced in the previous session."
  },
  {
    "objectID": "sections/pytorch-hands-on-lab.html#breakdown",
    "href": "sections/pytorch-hands-on-lab.html#breakdown",
    "title": "4  PyTorch Hands-on Lab",
    "section": "4.3 Breakdown:",
    "text": "4.3 Breakdown:\n\nTensor Operations Exercise\n\nTask: Create tensors, perform basic operations, and move tensors between CPU and GPU\n\nData Loading and Preprocessing Exercise\n\nTask: Load a small subset of RTS data using Dataset and DataLoader\n\nModel Building Exercise\n\nTask: Construct a simple neural network using nn.Module\n\nOptimization Exercise\n\nTask: Define a loss function, an optimizer, and a learning rate scheduler\n\nMini Training Loop Exercise\n\nTask: Implement a basic training loop to train the model on the small subset of RTS data\n\nDiscussion and Troubleshooting\n\nDiscuss potential improvements or extensions to the exercises"
  },
  {
    "objectID": "sections/AI-ready-training-datasets.html#synopsis",
    "href": "sections/AI-ready-training-datasets.html#synopsis",
    "title": "5  Design principles for an AI-ready training dataset",
    "section": "5.1 Synopsis",
    "text": "5.1 Synopsis\nIntroduce principles, approaches, tools, and strategies to create a high- quality AI-ready training dataset that is diverse, sizable, representative, and minimizes data bias for thoughtful AI research."
  },
  {
    "objectID": "sections/AI-ready-training-datasets.html#learning-outcome",
    "href": "sections/AI-ready-training-datasets.html#learning-outcome",
    "title": "5  Design principles for an AI-ready training dataset",
    "section": "5.2 Learning outcome",
    "text": "5.2 Learning outcome\nTrainees will become familiar with tools for training data creation and gain skills to correctly annotate and document training data and share the data with the broader research community."
  },
  {
    "objectID": "sections/AI-ready-training-datasets.html#ai-tools",
    "href": "sections/AI-ready-training-datasets.html#ai-tools",
    "title": "5  Design principles for an AI-ready training dataset",
    "section": "5.3 AI tools",
    "text": "5.3 AI tools\nCVAT (Computer Vision Annotation Tool), PDG data annotation platform\n\n\n\nRef: https://github.com/opencv/cvat"
  },
  {
    "objectID": "sections/data-anotation-for-deep-learning.html#notes",
    "href": "sections/data-anotation-for-deep-learning.html#notes",
    "title": "6  Data annotations for deep learning",
    "section": "6.1 Notes",
    "text": "6.1 Notes\n2 hours"
  },
  {
    "objectID": "sections/data-anotation-for-deep-learning.html#goal",
    "href": "sections/data-anotation-for-deep-learning.html#goal",
    "title": "6  Data annotations for deep learning",
    "section": "6.2 Goal",
    "text": "6.2 Goal\nTo provide participants with a comprehensive understanding of the importance of training data, methods to obtain it, tools for annotation, and potential data sources."
  },
  {
    "objectID": "sections/data-anotation-for-deep-learning.html#breakdown",
    "href": "sections/data-anotation-for-deep-learning.html#breakdown",
    "title": "6  Data annotations for deep learning",
    "section": "6.3 Breakdown",
    "text": "6.3 Breakdown\n\nIntroduction to Training Data\n\nWhat is training data and why is it crucial?\nDifferences between labeled and unlabeled data\n\nThe Importance of Quality Annotations\n\nHow annotations impact model performance\nCommon challenges: Inconsistent annotations, class imbalance, etc.\nStrategies to ensure high-quality annotations: Guidelines, multiple annotators, quality checks\n\nMethods to Obtain Training Data\n\nCreating your own dataset: Pros, cons, and considerations\nUsing pre-existing datasets: Benefits and potential pitfalls\nData augmentation: Expanding dataset size and diversity\nTransfer learning and pre-trained models: Leveraging external knowledge\n\nAnnotation Tools\n\nOverview of popular annotation tools: Labelbox, VGG Image Annotator (VIA), RectLabel, etc.\nFeatures to consider: Collaboration, format export options, automation capabilities\nHands-on demo: Annotating a sample image using a chosen tool\n\nData Sources for RTS and Arctic Science (15 minutes)\n\nPublic datasets relevant to arctic science and RTS\nCollaborative efforts and data-sharing initiatives in the research community\nEthical considerations: lesson 12\n\nQ&A and Discussion\n\nEncouraging sharing of personal experiences or challenges with data annotation\nDiscussing potential future developments in annotation tools and techniques"
  },
  {
    "objectID": "sections/u-net-for-semanting-segmentation.html#notes",
    "href": "sections/u-net-for-semanting-segmentation.html#notes",
    "title": "7  Deep dive into U-Net for semantic segmentation",
    "section": "7.1 Notes",
    "text": "7.1 Notes\n2 hours"
  },
  {
    "objectID": "sections/u-net-for-semanting-segmentation.html#goal",
    "href": "sections/u-net-for-semanting-segmentation.html#goal",
    "title": "7  Deep dive into U-Net for semantic segmentation",
    "section": "7.2 Goal",
    "text": "7.2 Goal\nTo provide participants with a deep understanding of the U-Net architecture, its relevance to semantic segmentation tasks, and its application for RTS mapping."
  },
  {
    "objectID": "sections/u-net-for-semanting-segmentation.html#breakdown",
    "href": "sections/u-net-for-semanting-segmentation.html#breakdown",
    "title": "7  Deep dive into U-Net for semantic segmentation",
    "section": "7.3 Breakdown",
    "text": "7.3 Breakdown\n\nIntroduction to Semantic Segmentation\n\nDefinition and significance of semantic segmentation\nDifferences between classification, object detection, and segmentation\nRelevance to RTS mapping\n\nOverview of U-Net Architecture\n\nHistorical context: Why and where was U-Net developed?\nKey features of U-Net: Symmetry, skip connections, etc.\nVisual representation of U-Net’s architecture\n\nEssentials of U-Net Components\n\nContracting path and its role in feature extraction\nBottleneck: Capturing the context\nExpansive path: Localizing features using skip connections\n\nIntroduction to Other Semantic Segmentation Models\n\nFCN (Fully Convolutional Network): The pioneer in end-to-end segmentation\nSegNet: Architecture with encoder-decoder structure\n…\n\nCase Study: U-Net for RTS Mapping\n\nWalkthrough of a real-world application of U-Net for RTS mapping\nVisualization of segmentation results on RTS data"
  },
  {
    "objectID": "sections/u-net-lab.html#goal",
    "href": "sections/u-net-lab.html#goal",
    "title": "8  U-Net for RTS mapping hands-on lab",
    "section": "8.1 Goal",
    "text": "8.1 Goal\nTo provide participants with practical experience in implementing and experimenting with the U-Net architecture for semantic segmentation on RTS data."
  },
  {
    "objectID": "sections/u-net-lab.html#breakdown",
    "href": "sections/u-net-lab.html#breakdown",
    "title": "8  U-Net for RTS mapping hands-on lab",
    "section": "8.2 Breakdown",
    "text": "8.2 Breakdown\n\nData Loading\n\nTask: Loading RTS data for the lab\n\nImplementing U-Net Architecture\n\nTask: Define the U-Net architecture using nn.Module in PyTorch\nGuided step-by-step construction of the contracting path, bottleneck, and expansive path\nTips: Emphasize the importance of matching tensor dimensions\n\nDefining the Loss Function and Optimizer\n\nTask: Choose an appropriate loss function for segmentation (e.g., Dice loss, cross-entropy loss)\nSet up an optimizer (e.g., Adam) for training\n\nMini Training Loop\n\nTask: Implement a basic training loop to train the U-Net model on the RTS data subset\nMonitor the loss and visualize some predictions after a few epochs\nTips: Discuss the importance of data augmentation and learning rate choices for segmentation tasks\n\nDiscussion and Troubleshooting\n\nShare insights or observations from the training process\nEncourage participants to discuss their experiences and any modifications they tried"
  },
  {
    "objectID": "sections/model-explainability.html#goal",
    "href": "sections/model-explainability.html#goal",
    "title": "9  Model explainability and scientific soundness",
    "section": "9.1 Goal",
    "text": "9.1 Goal\nIntroduce the black box nature of AI models, the importance of interpretability and transparency of AI models (e.g., safety, security, and bias in models and datasets) and methods proposed to explain or reveal the ways that AI models make decisions."
  },
  {
    "objectID": "sections/model-explainability.html#breakdown",
    "href": "sections/model-explainability.html#breakdown",
    "title": "9  Model explainability and scientific soundness",
    "section": "9.2 Breakdown",
    "text": "9.2 Breakdown\n\nThe Black Box Dilemma\n\nWhy deep learning models are often perceived as “black boxes”\nThe importance of transparency and interpretability in scientific applications\n\nPrinciples of Scientific Soundness\n\nReproducibility: Ensuring experiments can be replicated by others\nRobustness: Model performance across different datasets and conditions\nGeneralizability: How well models perform on unseen data\n\nIntroduction to Model Explainability\n\nWhat is model explainability and why is it crucial?\nDifferences between global and local explainability\n\nTechniques for Model Interpretation\n\nFeature Visualization: Understanding what features a model has learned\nSaliency Maps: Highlighting important regions in input data\nActivation Maximization: Visualizing what maximally activates certain neurons\nSHAP (SHapley Additive exPlanations): Game theoretic approach to explain output of any machine learning model\n\nEnsuring Scientific Soundness in Deep Learning\n\nData integrity: Ensuring data quality and addressing biases\nModel validation: Techniques beyond traditional train-test splits (e.g., k-fold cross-validation)\nUncertainty quantification: Understanding and communicating model uncertainty\n\nCase Studies: Failures and Successes\n\nReal-world examples where lack of explainability or scientific rigor led to issues\nSuccess stories where proper model interpretation and validation made a difference\n\nQ&A and Discussion Encouraging sharing of personal experiences or challenges related to model explainability and scientific soundness Discussing potential future developments in the field of model interpretability"
  },
  {
    "objectID": "sections/model-explainability.html#ai-tools",
    "href": "sections/model-explainability.html#ai-tools",
    "title": "9  Model explainability and scientific soundness",
    "section": "9.3 AI tools",
    "text": "9.3 AI tools\nZetane viewer (an open-source AI model explanation tool)\n\n\n\nSource: https://zetane.com/gallery"
  }
]